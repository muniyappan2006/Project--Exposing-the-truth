# -*- coding: utf-8 -*-
"""Phase_2_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lyl1SMCaxnQqEM3CpTbUltSDXJuGebNv

upload the dataset
"""

from google.colab import files
uploaded = files.upload()

"""load the dataset"""

import pandas as pd
import os

# Check if the file exists
if os.path.exists("fake_and_real_news(1).csv"):
  # Read the dataset if the file exists
  df = pd.read_csv("fake_and_real_news(1).csv", sep=';')
else:
  # If the file is not found, print an error message and instructions
  print("Error: The file 'fake_and_real_news(1).csv' was not found.")
  print("Please upload the file using 'files.upload()' before running this code.")

"""data exploration"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Display basic info
print("Dataset Shape:", df.shape)
print("\nColumn Names:", df.columns.tolist())
print("\nMissing Values:\n", df.isnull().sum())
print("\nData Types:\n", df.dtypes)
print("\nSample Data:\n", df.head())

# Label distribution
label_counts = df['label'].value_counts()
print("\nLabel Distribution:\n", label_counts)

# Plot label distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='label', palette='Set2')
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Add a column for text length
df['text_length'] = df['Text'].apply(len)

# Summary statistics for text length
print("\nText Length Statistics:\n", df['text_length'].describe())

# Plot text length distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['text_length'], bins=50, kde=True, color='skyblue')
plt.title('Text Length Distribution')
plt.xlabel('Length of Text')
plt.ylabel('Frequency')
plt.show()

"""check foir missing values and duplicates"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Get the current working directory
current_directory = os.getcwd()
print("Current working directory:", current_directory)

# Construct the full file path
file_path = os.path.join(current_directory, 'fake_and_real_news(1).csv')

# Check if the file exists at the specified path
if os.path.exists(file_path):
    # Load the dataset
    df = pd.read_csv(file_path)
else:
    # If the file is not found, print an error message and instructions
    print(f"Error: The file 'fake_and_real_news(1).csv' was not found at '{file_path}'.")
    print("Please make sure the file is in the current working directory or provide the correct file path.")
    exit()  # Exit the script

"""visualize a few features"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Add features
df['text_length'] = df['Text'].apply(len)
df['word_count'] = df['Text'].apply(lambda x: len(str(x).split()))

# Set visual style
sns.set(style="whitegrid")

# 1. Label distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='label', data=df, palette='Set1')
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# 2. Text length distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['text_length'], bins=50, color='blue', kde=True)
plt.title('Text Length Distribution')
plt.xlabel('Text Length (characters)')
plt.ylabel('Frequency')
plt.show()

# 3. Word count distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['word_count'], bins=50, color='green', kde=True)
plt.title('Word Count Distribution')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()

"""identify target and features"""

import pandas as pd

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Identify features and target
X = df['Text']        # Feature (news text)
y = df['label']       # Target (Fake or Real)

print("Feature sample:\n", X.head())
print("\nTarget sample:\n", y.head())

"""convert Categorical Column to Numerical"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Initialize the encoder
le = LabelEncoder()

# Convert 'label' column to numerical values
df['label_encoded'] = le.fit_transform(df['label'])

# Show label mapping
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("Label Mapping:", label_mapping)

# Preview result
print(df[['label', 'label_encoded']].head())

"""Onr hot Encoding"""

import pandas as pd

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# One-Hot Encode the 'label' column
one_hot = pd.get_dummies(df['label'], prefix='label')

# Concatenate back to the original DataFrame
df_encoded = pd.concat([df, one_hot], axis=1)

# Preview result
print(df_encoded[['label'] + one_hot.columns.tolist()].head())

"""Feature Scaling"""

from sklearn.preprocessing import StandardScaler

# Example: using 'text_length' and 'word_count'
df['text_length'] = df['Text'].apply(len)
df['word_count'] = df['Text'].apply(lambda x: len(x.split()))

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['text_length', 'word_count']])

print("Scaled Feature Sample:\n", scaled_features[:5])

"""Train-Test Split"""

# Import necessary libraries
from sklearn.model_selection import train_test_split
import numpy as np

# Example data (features and labels)
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features
y = np.array([1, 2, 3, 4, 5])  # Labels

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the results
print("Training Features:\n", X_train)
print("Testing Features:\n", X_test)
print("Training Labels:\n", y_train)
print("Testing Labels:\n", y_test)

"""Model building"""

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the model (Decision Tree Classifier in this case)
model = DecisionTreeClassifier(random_state=42)

# Train the model on the training data
model.fit(X_train, y_train)

# Predict the labels on the test set
y_pred = model.predict(X_test)

# Evaluate the model performance using accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print the results
print("Accuracy of the model:", accuracy)
print("\nPredicted labels:", y_pred)

"""Evalution"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files # Import files from google.colab

# Upload the 'fake_news.csv' file
uploaded = files.upload()

# Load your dataset (assuming a CSV file with 'text' and 'label' columns)
# Replace 'fake_news.csv' with your actual dataset file path if it's different
data = pd.read_csv('fake_and_real_news (1).csv')

# ... (rest of your code remains the same)

"""Make Prediction from New Input"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('fake_and_real_news (1).csv')  # Replace with your actual dataset path, with the space

# Split data into training and testing sets (adjust test_size and random_state if needed)
X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['label'], test_size=0.2, random_state=42)

"""Convert to DAtaFrame and Encode"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Assume you already have the trained model and vectorizer from previous steps
# model = LogisticRegression(random_state=42)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Initialize the vectorizer

# Sample new input to predict
new_input = [
    "The government has announced a new initiative to improve education in rural areas.",
    "Scientists claim to have discovered proof of extraterrestrial life on Mars, a groundbreaking revelation."
]

# Fit the vectorizer to your data or some sample data first.
# Here, we are fitting it to the new_input itself, but ideally you would fit it to your training data.
vectorizer.fit(new_input) # Fit the vectorizer

# Preprocess the new input (vectorize it using the same vectorizer that was used to train the model)
new_input_tfidf = vectorizer.transform(new_input)

# Use the trained model to make predictions
# Assuming you have a trained model named 'model' from previous steps
# predictions = model.predict(new_input_tfidf)

# Convert to DataFrame (assuming predictions are available)
# results = pd.DataFrame({
#     'News Article': new_input,
#     'Prediction': ['Fake News' if pred == 1 else 'Real News' for pred in predictions],
#     'Prediction (Encoded)': predictions  # 1 for Fake News, 0 for Real News
# })

# Display the DataFrame
# print(results)
print(new_input_tfidf) # Print the transformed input instead of results

"""Predict the Final Grade"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Sample dataset: Replace with your actual dataset
# The dataset should have 'text' (news articles) and 'label' (0 for real news, 1 for fake news)
data = {
    'text': [
        "The president has announced a new economic plan.",
        "Breaking news: Aliens have landed on Earth!",
        "New research shows benefits of daily exercise.",
        "The government is covering up the truth about the pandemic.",
        "Local schools report a rise in student performance."
    ],
    'label': [0, 1, 0, 1, 0]  # 0 = Real news, 1 = Fake news
}

# Create a DataFrame
df = pd.DataFrame(data)

# Text and label data
X = df['text']
y = df['label']

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the text using TF-IDF (Term Frequency-Inverse Document Frequency)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Initialize and train a Logistic Regression model for classification
model = LogisticRegression(random_state=42)
model.fit(X_train_tfidf, y_train)

# Predict the labels (real/fake) on the test data
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# If you want to predict a 'final grade' score, let's assume we want to predict a confidence score
# The model will output the probabilities of each article being fake (1) or real (0)
y_pred_prob = model.predict_proba(X_test_tfidf)[:, 1]  # Probability of being fake (1)

# For the 'final grade', we use the probability as a score
final_grades_df = pd.DataFrame({
    'News Article': X_test,
    'Predicted Label': ['Fake News' if pred == 1 else 'Real News' for pred in y_pred],
    'Confidence Score': y_pred_prob  # The predicted probability as a final grade (score)
})

# Display the DataFrame with news articles, predictions, and confidence scores
print(final_grades_df)

# Example of predicting the final grade for a new article
new_article = ["Aliens invade New York City in the latest government cover-up."]
new_article_tfidf = vectorizer.transform(new_article)
predicted_prob = model.predict_proba(new_article_tfidf)[:, 1]
predicted_grade = predicted_prob[0]

print(f"\nNew Article: {new_article[0]}")
print(f"Predicted Final Grade (Confidence Score): {predicted_grade:.2f}")

"""Deployment- Building an Interactive App"""

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Assume you have a trained model and vectorizer
# model = LogisticRegression()
# vectorizer = TfidfVectorizer()

# Save the trained model and vectorizer
joblib.dump(model, 'fake_news_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

pip install pandas scikit-learn gradio

"""Create the gradio interface"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import gradio as gr

# Step 1: Load and preprocess dataset
# Make sure the correct file name is used. The traceback shows 'fake_and_real_news.csv'.
# If the file is 'fake_and_real_news(1).csv' with a space, adjust the filename.
df = pd.read_csv("fake_and_real_news.csv") # or pd.read_csv("fake_and_real_news(1).csv") if that's the file

# Use only needed columns. Based on the error and global variables, columns are 'Text' and 'label'.
df = df[['Text', 'label']].dropna() # Changed 'title' and 'text' to 'Text'

# There is no 'title' column, so we just use the 'Text' column as the content.
# If your dataset truly has a 'title' and 'text' column, ensure you are loading the correct file.
# Assuming 'Text' is the main content column.
df['content'] = df['Text']

# Step 2: Train/Test split
X = df['content']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create a text classification pipeline
model = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english', max_df=0.7)),
    ('clf', LogisticRegression(solver='liblinear'))
])

# Step 4: Train the model
model.fit(X_train, y_train)

# Step 5: Define prediction function
def predict_news(text):
    if not text.strip():
        return "Please enter some text to classify."
    pred = model.predict([text])[0]
    # Predict_proba returns probabilities for all classes. We need the probability of the predicted class.
    # Find the index of the predicted class in model.classes_
    pred_index = list(model.classes_).index(pred)
    prob = model.predict_proba([text])[0][pred_index]

    return f"This news is **{pred.upper()}** (Confidence: {prob:.2f})"

# Step 6: Build Gradio interface
with gr.Blocks(title="Fake News Detection App") as demo:
    gr.Markdown("## ðŸ“° Fake News Detection App")
    gr.Markdown("Enter a news article or headline to check if it's likely **REAL** or **FAKE**.")

    with gr.Row():
        text_input = gr.Textbox(label="News Text", lines=10, placeholder="Paste or type news content here...")
        output = gr.Textbox(label="Prediction")

    btn = gr.Button("Classify")
    btn.click(fn=predict_news, inputs=text_input, outputs=output)

# Step 7: Launch the app
demo.launch()