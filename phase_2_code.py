# -*- coding: utf-8 -*-
"""Phase_2_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J6IQ6eUjq5JJU5E8c9_g7cT1bQaPWLDx
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import os

# Check if the file exists
if os.path.exists("fake_and_real_news(1).csv"):
  # Read the dataset if the file exists
  df = pd.read_csv("fake_and_real_news(1).csv", sep=';')
else:
  # If the file is not found, print an error message and instructions
  print("Error: The file 'fake_and_real_news(1).csv' was not found.")
  print("Please upload the file using 'files.upload()' before running this code.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Display basic info
print("Dataset Shape:", df.shape)
print("\nColumn Names:", df.columns.tolist())
print("\nMissing Values:\n", df.isnull().sum())
print("\nData Types:\n", df.dtypes)
print("\nSample Data:\n", df.head())

# Label distribution
label_counts = df['label'].value_counts()
print("\nLabel Distribution:\n", label_counts)

# Plot label distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='label', palette='Set2')
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Add a column for text length
df['text_length'] = df['Text'].apply(len)

# Summary statistics for text length
print("\nText Length Statistics:\n", df['text_length'].describe())

# Plot text length distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['text_length'], bins=50, kde=True, color='skyblue')
plt.title('Text Length Distribution')
plt.xlabel('Length of Text')
plt.ylabel('Frequency')
plt.show()

import pandas as pd

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Check for missing values
print("Missing Values in Each Column:\n", df.isnull().sum())

# Check for duplicates
duplicate_rows = df.duplicated()
print(f"\nNumber of Duplicate Rows: {duplicate_rows.sum()}")

# Optionally display duplicate rows
if duplicate_rows.sum() > 0:
    print("\nSample Duplicate Rows:\n", df[duplicate_rows].head())

# Optionally remove duplicates
# df = df.drop_duplicates()
# print("\nDuplicates removed. New shape:", df.shape)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Add features
df['text_length'] = df['Text'].apply(len)
df['word_count'] = df['Text'].apply(lambda x: len(str(x).split()))

# Set visual style
sns.set(style="whitegrid")

# 1. Label distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='label', data=df, palette='Set1')
plt.title('Label Distribution')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# 2. Text length distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['text_length'], bins=50, color='blue', kde=True)
plt.title('Text Length Distribution')
plt.xlabel('Text Length (characters)')
plt.ylabel('Frequency')
plt.show()

# 3. Word count distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['word_count'], bins=50, color='green', kde=True)
plt.title('Word Count Distribution')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()

import pandas as pd

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Identify features and target
X = df['Text']        # Feature (news text)
y = df['label']       # Target (Fake or Real)

print("Feature sample:\n", X.head())
print("\nTarget sample:\n", y.head())

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# Initialize the encoder
le = LabelEncoder()

# Convert 'label' column to numerical values
df['label_encoded'] = le.fit_transform(df['label'])

# Show label mapping
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("Label Mapping:", label_mapping)

# Preview result
print(df[['label', 'label_encoded']].head())

import pandas as pd

# Load the dataset
df = pd.read_csv('fake_and_real_news.csv')

# One-Hot Encode the 'label' column
one_hot = pd.get_dummies(df['label'], prefix='label')

# Concatenate back to the original DataFrame
df_encoded = pd.concat([df, one_hot], axis=1)

# Preview result
print(df_encoded[['label'] + one_hot.columns.tolist()].head())

from sklearn.preprocessing import StandardScaler

# Example: using 'text_length' and 'word_count'
df['text_length'] = df['Text'].apply(len)
df['word_count'] = df['Text'].apply(lambda x: len(x.split()))

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['text_length', 'word_count']])

print("Scaled Feature Sample:\n", scaled_features[:5])

# Import necessary libraries
from sklearn.model_selection import train_test_split
import numpy as np

# Example data (features and labels)
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features
y = np.array([1, 2, 3, 4, 5])  # Labels

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the results
print("Training Features:\n", X_train)
print("Testing Features:\n", X_test)
print("Training Labels:\n", y_train)
print("Testing Labels:\n", y_test)

# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the model (Decision Tree Classifier in this case)
model = DecisionTreeClassifier(random_state=42)

# Train the model on the training data
model.fit(X_train, y_train)

# Predict the labels on the test set
y_pred = model.predict(X_test)

# Evaluate the model performance using accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print the results
print("Accuracy of the model:", accuracy)
print("\nPredicted labels:", y_pred)

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files # Import files from google.colab

# Upload the 'fake_news.csv' file
uploaded = files.upload()

# Load your dataset (assuming a CSV file with 'text' and 'label' columns)
# Replace 'fake_news.csv' with your actual dataset file path if it's different
data = pd.read_csv('fake_and_real_news (1).csv')

# ... (rest of your code remains the same)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('fake_and_real_news (1).csv')  # Replace with your actual dataset path, with the space

# Split data into training and testing sets (adjust test_size and random_state if needed)
X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['label'], test_size=0.2, random_state=42)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Assume you already have the trained model and vectorizer from previous steps
# model = LogisticRegression(random_state=42)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000) # Initialize the vectorizer

# Sample new input to predict
new_input = [
    "The government has announced a new initiative to improve education in rural areas.",
    "Scientists claim to have discovered proof of extraterrestrial life on Mars, a groundbreaking revelation."
]

# Fit the vectorizer to your data or some sample data first.
# Here, we are fitting it to the new_input itself, but ideally you would fit it to your training data.
vectorizer.fit(new_input) # Fit the vectorizer

# Preprocess the new input (vectorize it using the same vectorizer that was used to train the model)
new_input_tfidf = vectorizer.transform(new_input)

# Use the trained model to make predictions
# Assuming you have a trained model named 'model' from previous steps
# predictions = model.predict(new_input_tfidf)

# Convert to DataFrame (assuming predictions are available)
# results = pd.DataFrame({
#     'News Article': new_input,
#     'Prediction': ['Fake News' if pred == 1 else 'Real News' for pred in predictions],
#     'Prediction (Encoded)': predictions  # 1 for Fake News, 0 for Real News
# })

# Display the DataFrame
# print(results)
print(new_input_tfidf) # Print the transformed input instead of results

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Sample dataset: Replace with your actual dataset
# The dataset should have 'text' (news articles) and 'label' (0 for real news, 1 for fake news)
data = {
    'text': [
        "The president has announced a new economic plan.",
        "Breaking news: Aliens have landed on Earth!",
        "New research shows benefits of daily exercise.",
        "The government is covering up the truth about the pandemic.",
        "Local schools report a rise in student performance."
    ],
    'label': [0, 1, 0, 1, 0]  # 0 = Real news, 1 = Fake news
}

# Create a DataFrame
df = pd.DataFrame(data)

# Text and label data
X = df['text']
y = df['label']

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess the text using TF-IDF (Term Frequency-Inverse Document Frequency)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Initialize and train a Logistic Regression model for classification
model = LogisticRegression(random_state=42)
model.fit(X_train_tfidf, y_train)

# Predict the labels (real/fake) on the test data
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# If you want to predict a 'final grade' score, let's assume we want to predict a confidence score
# The model will output the probabilities of each article being fake (1) or real (0)
y_pred_prob = model.predict_proba(X_test_tfidf)[:, 1]  # Probability of being fake (1)

# For the 'final grade', we use the probability as a score
final_grades_df = pd.DataFrame({
    'News Article': X_test,
    'Predicted Label': ['Fake News' if pred == 1 else 'Real News' for pred in y_pred],
    'Confidence Score': y_pred_prob  # The predicted probability as a final grade (score)
})

# Display the DataFrame with news articles, predictions, and confidence scores
print(final_grades_df)

# Example of predicting the final grade for a new article
new_article = ["Aliens invade New York City in the latest government cover-up."]
new_article_tfidf = vectorizer.transform(new_article)
predicted_prob = model.predict_proba(new_article_tfidf)[:, 1]
predicted_grade = predicted_prob[0]

print(f"\nNew Article: {new_article[0]}")
print(f"Predicted Final Grade (Confidence Score): {predicted_grade:.2f}")

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Assume you have a trained model and vectorizer
# model = LogisticRegression()
# vectorizer = TfidfVectorizer()

# Save the trained model and vectorizer
joblib.dump(model, 'fake_news_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')